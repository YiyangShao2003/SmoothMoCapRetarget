{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data corresponding to text caption\n",
    "\n",
    "## 1. Text captions are stored in the 'texts' directory unziped from the 'texts.zip' file, which should look like:\n",
    "\n",
    "texts\\\n",
    "├── 000000.txt\\\n",
    "├── 000001.txt\\\n",
    "├── ...\\\n",
    "├── 014615.txt\\\n",
    "├── M000000.txt\\\n",
    "├── M000001.txt\\\n",
    "├── ...\\\n",
    "└── M014615.txt\\\n",
    "\n",
    "where for example 000000.txt should contain:\n",
    "\n",
    "`\n",
    "a man kicks something or someone with his left leg.#a/DET man/NOUN kick/VERB something/PRON or/CCONJ someone/PRON with/ADP his/DET left/ADJ leg/NOUN#0.0#0.0\n",
    "the standing person kicks with their left foot before going back to their original stance.#the/DET stand/VERB person/NOUN kick/VERB with/ADP their/DET left/ADJ foot/NOUN before/ADP go/VERB back/ADV to/ADP their/DET original/ADJ stance/NOUN#0.0#0.0\n",
    "a man kicks with something or someone with his left leg.#a/DET man/NOUN kick/VERB with/ADP something/PRON or/CCONJ someone/PRON with/ADP his/DET left/ADJ leg/NOUN#0.0#0.0\n",
    "he is flying kick with his left leg#he/PRON is/AUX fly/VERB kick/NOUN with/ADP his/DET left/ADJ leg/NOUN#0.0#0.0\n",
    "`\n",
    "\n",
    "and M000000.txt should contain:\n",
    "\n",
    "`\n",
    "a man kicks something or someone with his right leg.#a/DET man/NOUN kick/VERB something/PRON or/CCONJ someone/PRON with/ADP his/DET right/ADJ leg/NOUN#0.0#0.0\n",
    "the standing person kicks with their right foot before going back to their original stance.#the/DET stand/VERB person/NOUN kick/VERB with/ADP their/DET right/ADJ foot/NOUN before/ADP go/VERB back/ADV to/ADP their/DET original/ADJ stance/NOUN#0.0#0.0\n",
    "a man kicks with something or someone with his right leg.#a/DET man/NOUN kick/VERB with/ADP something/PRON or/CCONJ someone/PRON with/ADP his/DET right/ADJ leg/NOUN#0.0#0.0\n",
    "he is flying kick with his right leg#he/PRON is/AUX fly/VERB kick/NOUN with/ADP his/DET right/ADJ leg/NOUN#0.0#0.0\n",
    "`\n",
    "\n",
    "## 2. By default, we assume AMASS dataset in under the directory 'data/AMASS/AMASS_Complete', which should look like as following under 'data':\n",
    "\n",
    "\n",
    "AMASS\\\n",
    "└─&nbsp;AMASS_Complete\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├─&nbsp;ACCAD\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;├─&nbsp;Female1General_c3d\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;├─&nbsp;A1 - Stand_poses.npz\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;├─&nbsp;...\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;└─&nbsp;A15 - skip to stand_poses.npz\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;├─&nbsp;...\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;└─&nbsp;s011\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├─&nbsp;BioMotionLab_NTroje\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;└─&nbsp;...\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├─&nbsp;...\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├─&nbsp;Transitions_mocap\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;└─&nbsp;...\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└─&nbsp;LICENSE.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# Load csv file containing motion data information corresponding to the text\n",
    "index_path = '../../resources/index.csv'\n",
    "index_file = pd.read_csv(index_path)\n",
    "# Read data in the order of the index file\n",
    "humanml3d_data = {}\n",
    "# for i in tqdm(range(index_file.shape[0])):\n",
    "for i in tqdm(range(0, 100)):\n",
    "    source_path = index_file.loc[i]['source_path']\n",
    "    \n",
    "    # TODO: set the dataset_name_index in the path according to the actual path in the 'index.csv' file\n",
    "    dataset_name_index = 4\n",
    "    names_in_path = source_path.split('/')\n",
    "    dataset_name = names_in_path[dataset_name_index]\n",
    "    subject_name = names_in_path[dataset_name_index + 1]\n",
    "    action_name = names_in_path[dataset_name_index + 2]\n",
    "    save_name = f\"{dataset_name}_{subject_name}_{action_name[:-4]}\" # This name should correspond to that in \"amass_copycat_occlusion_v3.pkl\" except from the '0-' in the beginning   \n",
    "        \n",
    "    # Skip the data of the humanact12 dataset, since it is not contained in standard AMASS dataset\n",
    "    if 'humanact12' in source_path:\n",
    "        continue\n",
    "    \n",
    "    source_path = f\"../../{source_path}\"\n",
    "        \n",
    "    new_name = index_file.loc[i]['new_name']\n",
    "    data = np.load(source_path)\n",
    "    start_frame = index_file.loc[i]['start_frame']\n",
    "    end_frame = index_file.loc[i]['end_frame']\n",
    "    \n",
    "    data = dict(np.load(source_path))\n",
    "    data['start_frame'] = start_frame\n",
    "    data['end_frame'] = end_frame\n",
    "    data['new_name'] = new_name\n",
    "    \n",
    "    \n",
    "    humanml3d_data[save_name] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert HumanML3D dataset into training dataset\n",
    "\n",
    "## 1. 'data/occlusion/amass_copycat_occlusion_v3.pkl' is available [here](https://drive.google.com/uc?id=1uzFkT2s_zVdnAohPWHOLFcyRDq372Fmc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from phc.smpllib.smpl_parser import SMPL_Parser\n",
    "from phc.utils.transform_utils import convert_aa_to_orth6d\n",
    "from phc.utils.flags import flags\n",
    "\n",
    "\n",
    "# read occlusion data\n",
    "occlusion_path = '../../data/occlusion/amass_copycat_occlusion_v3.pkl'\n",
    "occlusion_data = joblib.load(occlusion_path)\n",
    "\n",
    "# read smpl parsers\n",
    "smpl_parser_n = SMPL_Parser(model_path=\"../../data/smpl\", gender=\"neutral\", use_pca=False, create_transl=False)\n",
    "smpl_parser_m = SMPL_Parser(model_path=\"../../data/smpl\", gender=\"male\", use_pca=False, create_transl=False)\n",
    "smpl_parser_f = SMPL_Parser(model_path=\"../../data/smpl\", gender=\"female\", use_pca=False, create_transl=False)\n",
    "\n",
    "# parameters\n",
    "target_frequency = 30\n",
    "flags.debug = False\n",
    "\n",
    "# HumanML3D parameters\n",
    "humanml3d_frequency = 20\n",
    "skip_frames = {'Eyes_Japan_Dataset' : 3 * target_frequency,     # 3 seconds\n",
    "               'MPI_HDM05' : 3 * target_frequency,              # 3 seconds\n",
    "               'TotalCapture' : 1 * target_frequency,           # 1 second\n",
    "               'MPI_Limits' : 1 * target_frequency,             # 1 second\n",
    "               'Transitions_mocap' : int(0.5 * target_frequency)# 0.5 second\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# functions to process data\n",
    "def fix_height_smpl_vanilla(pose_aa, th_trans, th_betas, gender):\n",
    "    # no filtering, just fix height. Make sure that the lowest point is greater or equal to 0, implying that the person is above the ground\n",
    "    gender = gender.item() if isinstance(gender, np.ndarray) else gender\n",
    "    if isinstance(gender, bytes):\n",
    "        gender = gender.decode(\"utf-8\")\n",
    "\n",
    "    if gender == \"neutral\":\n",
    "        smpl_parser = smpl_parser_n\n",
    "    elif gender == \"male\":\n",
    "        smpl_parser = smpl_parser_m\n",
    "    elif gender == \"female\":\n",
    "        smpl_parser = smpl_parser_f\n",
    "    else:\n",
    "        print(gender)\n",
    "        raise Exception(\"Gender Not Supported!!\")\n",
    "\n",
    "    verts, jts = smpl_parser.get_joints_verts(pose_aa[0:1], th_betas.repeat((1, 1)), th_trans=th_trans[0:1])\n",
    "\n",
    "    ground_position = torch.min(verts[:, :, 2])\n",
    "\n",
    "    # if ground_position < 0:\n",
    "    th_trans[:, 2] -= ground_position\n",
    "\n",
    "    return th_trans\n",
    "\n",
    "def process_data_dict(data_dict):\n",
    "    amass_res = {}\n",
    "    removed_k = []\n",
    "    pbar = data_dict\n",
    "    for k, v in tqdm(pbar.items()):\n",
    "        k = \"0-\" + k\n",
    "        seq_name = k\n",
    "        new_name = v[\"new_name\"]\n",
    "        betas = v[\"betas\"]\n",
    "        gender = v[\"gender\"]\n",
    "        # downsample the sequence to the target frequency\n",
    "        amass_frequency = v[\"mocap_framerate\"]\n",
    "        skip = int(amass_frequency / target_frequency)\n",
    "        \n",
    "        amass_pose = v[\"poses\"][::skip]\n",
    "        amass_trans = v[\"trans\"][::skip]\n",
    "        # segment the sequence according to the start and end frame\n",
    "        start_frame_std_freq = v[\"start_frame\"]\n",
    "        end_frame_std_freq = v[\"end_frame\"]\n",
    "        start_frame = int(start_frame_std_freq * target_frequency / humanml3d_frequency)\n",
    "        end_frame = int(end_frame_std_freq * target_frequency / humanml3d_frequency)\n",
    "        for key, value in skip_frames.items():  # skip the first few frames if needed\n",
    "            if key in k:\n",
    "                start_frame = start_frame + value\n",
    "                end_frame = end_frame + value\n",
    "                break\n",
    "        amass_pose = amass_pose[start_frame:end_frame]\n",
    "        amass_trans = amass_trans[start_frame:end_frame]\n",
    "        \n",
    "        # check occlusion and skip those occluded sequences\n",
    "        if k in occlusion_data:\n",
    "            continue\n",
    "        \n",
    "        # if the sequence is too short, we skip the sequence\n",
    "        seq_length = amass_pose.shape[0]\n",
    "        if seq_length < 10:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            batch_size = amass_pose.shape[0]\n",
    "            amass_pose = np.concatenate([amass_pose[:, :66], np.zeros((batch_size, 6))], axis=1) # We use SMPL and not SMPLH, meaning that we don't use the hand joints data\n",
    "            \n",
    "            pose_aa = torch.tensor(amass_pose)\n",
    "            amass_trans = torch.tensor(amass_trans)\n",
    "            betas = torch.from_numpy(betas)\n",
    "\n",
    "            amass_trans = fix_height_smpl_vanilla(\n",
    "                pose_aa=pose_aa,\n",
    "                th_betas=betas,\n",
    "                th_trans=amass_trans,\n",
    "                gender=gender,\n",
    "            )\n",
    "\n",
    "            pose_seq_6d = convert_aa_to_orth6d(torch.tensor(pose_aa)).reshape(batch_size, -1, 6)\n",
    "\n",
    "            amass_res[new_name] = {\n",
    "                \"pose_aa\": pose_aa.numpy(),\n",
    "                \"pose_6d\": pose_seq_6d.numpy(),\n",
    "                \"trans\": amass_trans.numpy(),\n",
    "                \"beta\": betas.numpy(),\n",
    "                \"seq_name\": seq_name,\n",
    "                \"gender\": gender,\n",
    "            }\n",
    "\n",
    "        if flags.debug and len(amass_res) > 10:\n",
    "            break\n",
    "    print(removed_k)\n",
    "    return amass_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "train_data = process_data_dict(humanml3d_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retarget training data to match robot structure\n",
    "\n",
    "## 1. 'data/g1/optimized_shape_scale_g1.pkl' is generated by fit_robot_shape_g1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import sys\n",
    "from turtle import left\n",
    "\n",
    "from sympy import jacobi\n",
    "sys.path.append(os.getcwd())\n",
    "from networkx import dorogovtsev_goltsev_mendes_graph\n",
    "from phc.utils.torch_g1_humanoid_batch import Humanoid_Batch, G1_ROTATION_AXIS\n",
    "from scipy.spatial.transform import Rotation as sRot\n",
    "from torch.autograd import Variable\n",
    "from phc.smpllib.smpl_parser import (\n",
    "    SMPL_Parser,\n",
    "    SMPL_BONE_ORDER_NAMES, \n",
    ")\n",
    "\n",
    "# load smpl model optimized for robot\n",
    "device = (\n",
    "        torch.device(\"cuda\", index=0)\n",
    "        if torch.cuda.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "optimized_shape_scale = joblib.load(\"../../data/g1/optimized_shape_scale_g1.pkl\")\n",
    "shape_new = optimized_shape_scale[\"shape\"].to(device)\n",
    "optimized_scale = optimized_shape_scale[\"scale\"].to(device)\n",
    "# load smpl parser\n",
    "smpl_parser_n = SMPL_Parser(model_path=\"../../data/smpl\", gender=\"neutral\")\n",
    "smpl_parser_n.to(device)\n",
    "\n",
    "# load robot forward kinematics model\n",
    "robot_fk = Humanoid_Batch(device = device)\n",
    "\n",
    "# robot specific parameters\n",
    "robot_rotation_axis = G1_ROTATION_AXIS.to(device)\n",
    "\n",
    "\n",
    "robot_link_names = [\n",
    "    'pelvis',\n",
    "    'left_hip_pitch_link',\n",
    "    'left_hip_roll_link',\n",
    "    'left_hip_yaw_link',\n",
    "    'left_knee_link',\n",
    "    'left_ankle_pitch_link',\n",
    "    'left_ankle_roll_link',\n",
    "    'right_hip_pitch_link',\n",
    "    'right_hip_roll_link',\n",
    "    'right_hip_yaw_link',\n",
    "    'right_knee_link',\n",
    "    'right_ankle_pitch_link',\n",
    "    'right_ankle_roll_link',\n",
    "    'waist_yaw_link',\n",
    "    'waist_roll_link',\n",
    "    'torso_link',\n",
    "    'left_shoulder_pitch_link',\n",
    "    'left_shoulder_roll_link',\n",
    "    'left_shoulder_yaw_link',\n",
    "    'left_elbow_link',\n",
    "    'left_wrist_roll_link',\n",
    "    'left_wrist_pitch_link',\n",
    "    'left_wrist_yaw_link',\n",
    "    'right_shoulder_pitch_link',\n",
    "    'right_shoulder_roll_link',\n",
    "    'right_shoulder_yaw_link',\n",
    "    'right_elbow_link',\n",
    "    'right_wrist_roll_link',\n",
    "    'right_wrist_pitch_link',\n",
    "    'right_wrist_yaw_link',\n",
    "]\n",
    "\n",
    "robot_joint_names = [\n",
    "    \"left_hip_pitch_joint\",\n",
    "    \"left_hip_roll_joint\",\n",
    "    \"left_hip_yaw_joint\",\n",
    "    \"left_knee_joint\",\n",
    "    \"left_ankle_pitch_joint\",\n",
    "    \"left_ankle_roll_joint\",\n",
    "    \"right_hip_pitch_joint\",\n",
    "    \"right_hip_roll_joint\",\n",
    "    \"right_hip_yaw_joint\",\n",
    "    \"right_knee_joint\",\n",
    "    \"right_ankle_pitch_joint\",\n",
    "    \"right_ankle_roll_joint\",\n",
    "    \"waist_yaw_joint\",\n",
    "    \"waist_roll_joint\",\n",
    "    \"waist_pitch_joint\",\n",
    "    \"left_shoulder_pitch_joint\",\n",
    "    \"left_shoulder_roll_joint\",\n",
    "    \"left_shoulder_yaw_joint\",\n",
    "    \"left_elbow_joint\",\n",
    "    \"left_wrist_roll_joint\",\n",
    "    \"left_wrist_pitch_joint\",\n",
    "    \"left_wrist_yaw_joint\",\n",
    "    \"right_shoulder_pitch_joint\",\n",
    "    \"right_shoulder_roll_joint\",\n",
    "    \"right_shoulder_yaw_joint\",\n",
    "    \"right_elbow_joint\",\n",
    "    \"right_wrist_roll_joint\",\n",
    "    \"right_wrist_pitch_joint\",\n",
    "    \"right_wrist_yaw_joint\",\n",
    "]\n",
    "\n",
    "robot_link_pick = ['pelvis',\n",
    "    'left_hip_pitch_link',\n",
    "    'left_knee_link',\n",
    "    'left_ankle_roll_link',\n",
    "    'right_hip_pitch_link',\n",
    "    'right_knee_link',\n",
    "    'right_ankle_roll_link',\n",
    "    'left_shoulder_roll_link',\n",
    "    'left_elbow_link',\n",
    "    'left_wrist_yaw_link',\n",
    "    'right_shoulder_roll_link',\n",
    "    'right_elbow_link',\n",
    "    'right_wrist_yaw_link',\n",
    "]\n",
    "smpl_link_pick = [\n",
    "    \"Pelvis\",\n",
    "    \"L_Hip\",\n",
    "    \"L_Knee\",\n",
    "    \"L_Ankle\",\n",
    "    \"R_Hip\",\n",
    "    \"R_Knee\",\n",
    "    \"R_Ankle\",\n",
    "    \"L_Shoulder\",\n",
    "    \"L_Elbow\",\n",
    "    \"L_Wrist\",\n",
    "    \"R_Shoulder\",\n",
    "    \"R_Elbow\",\n",
    "    \"R_Wrist\",\n",
    "]\n",
    "\n",
    "locked_joints = [\n",
    "    \"waist_roll_joint\",\n",
    "    \"waist_pitch_joint\",\n",
    "    # \"left_wrist_pitch_joint\",\n",
    "    # \"right_wrist_pitch_joint\",\n",
    "]\n",
    "\n",
    "hands_link = [\"left_wrist_yaw_joint\", \"right_wrist_yaw_joint\"]\n",
    "\n",
    "robot_link_pick_idx = [ robot_link_names.index(j) for j in robot_link_pick]\n",
    "smpl_link_pick_idx = [SMPL_BONE_ORDER_NAMES.index(j) for j in smpl_link_pick]\n",
    "locked_joints_idx = [robot_joint_names.index(j) for j in locked_joints]\n",
    "hands_link_idx = [robot_joint_names.index(j) for j in hands_link]\n",
    "\n",
    "dict_joint_name_index = {}\n",
    "for index, name in enumerate(robot_joint_names):\n",
    "    dict_joint_name_index[name] = index\n",
    "\n",
    "dict_link_name_index = {}\n",
    "for index, name in enumerate(robot_link_names):\n",
    "    dict_link_name_index[name] = index\n",
    "    \n",
    "dict_smpl_link_name_index = {}\n",
    "for index, name in enumerate(SMPL_BONE_ORDER_NAMES):\n",
    "    dict_smpl_link_name_index[name] = index\n",
    "\n",
    "# list_selected_links = [\"pelvis\", \"LL_faa\", \"LR_faa\", \"AL_sfe\", \"AL_efe\", \"AL_waa\", \"AR_sfe\", \"AR_efe\", \"AR_waa\"]\n",
    "list_selected_links = [\"pelvis\", \"left_shoulder_roll_link\", \"left_elbow_link\", \"left_wrist_yaw_link\", \"left_ankle_roll_link\", \"right_shoulder_roll_link\", \"right_elbow_link\", \"right_wrist_yaw_link\", \"right_ankle_roll_link\"]\n",
    "list_selected_joints = ['waist_yaw_joint', \n",
    "                        'left_shoulder_pitch_joint', 'left_shoulder_roll_joint', 'left_shoulder_yaw_joint', \n",
    "                        'left_elbow_joint', \n",
    "                        'left_wrist_roll_joint', 'left_wrist_pitch_joint', 'left_wrist_yaw_joint', \n",
    "                        'right_shoulder_pitch_joint', 'right_shoulder_roll_joint', 'right_shoulder_yaw_joint',\n",
    "                        'right_elbow_joint',\n",
    "                        'right_wrist_roll_joint', 'right_wrist_pitch_joint', 'right_wrist_yaw_joint']\n",
    "\n",
    "initial_joint_position_dict = {\n",
    "    # \"left_elbow_joint\": 1.57,\n",
    "    # \"right_elbow_joint\": 1.57,\n",
    "}\n",
    "\n",
    "# helper functions\n",
    "\n",
    "# get hand orientation\n",
    "def compute_hand_global_orientations(smpl_parser: SMPL_Parser, pose: torch.Tensor) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the global rotation matrices of the left and right hands.\n",
    "\n",
    "    param:\n",
    "        smpl_parser (SMPL_Parser)\n",
    "        pose (torch.Tensor): shape (batch_size, J*3)\n",
    "\n",
    "    返回:\n",
    "        left_hand_rotmat (torch.Tensor): global rotation matrix for left hand, shape: (batch_size, 3, 3)\n",
    "        right_hand_rotmat (torch.Tensor): global rotation matrix for right hand, shape: (batch_size, 3, 3)\n",
    "    \"\"\"\n",
    "    global_rotmats = smpl_parser.get_global_orientations(pose)  # (B, J, 3, 3)\n",
    "\n",
    "    # Get the indices of the left and right hands\n",
    "    left_hand_idx = SMPL_BONE_ORDER_NAMES.index(\"L_Hand\")\n",
    "    right_hand_idx = SMPL_BONE_ORDER_NAMES.index(\"R_Hand\")\n",
    "\n",
    "    left_hand_rotmat = global_rotmats[:, left_hand_idx]  # (B, 3, 3)\n",
    "    right_hand_rotmat = global_rotmats[:, right_hand_idx]  # (B, 3, 3)\n",
    "    \n",
    "    #  multiple the result bt an additional constant rotation matrix to align the hand with the robot hand\n",
    "    additional_rotmat_L = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, -1]], device=device).float() # y axis flip\n",
    "    additional_rotmat_R = torch.tensor([[0, -1, 0], [-1, 0, 0], [0, 0, -1]], device=device).float() # another y axis flip\n",
    "    # additional_rotmat = torch.tensor([[1, 0, 0], [0, 0, 1], [0, -1, 0]], device=device).float() # x axis flip\n",
    "    # additional_rotmat = torch.tensor([[1, 0, 0], [0, 0, -1], [0, 1, 0]], device=device).float() # another x axis flip\n",
    "    # additional_rotmat = torch.tensor([[0, 1, 0], [-1, 0, 0], [0, 0, 1]], device=device).float() # z axis flip\n",
    "    # additional_rotmat = torch.tensor([[0, -1, 0], [1, 0, 0], [0, 0, 1]], device=device).float() # another z axis flip\n",
    "    \n",
    "    left_hand_rotmat = left_hand_rotmat @ additional_rotmat_L\n",
    "    right_hand_rotmat = right_hand_rotmat @ additional_rotmat_R\n",
    "    \n",
    "    # transpose the rotation matrix to align with the robot hand\n",
    "    # left_hand_rotmat = left_hand_rotmat.transpose(1, 2)\n",
    "    # right_hand_rotmat = right_hand_rotmat.transpose(1, 2)\n",
    "\n",
    "    return left_hand_rotmat, right_hand_rotmat\n",
    "\n",
    "def vee(skew_tensor):\n",
    "    \"\"\"\n",
    "    Convert a skew-symmetric matrix to a vector.\n",
    "\n",
    "    Args:\n",
    "        skew_tensor (torch.Tensor): [batch_size, num_links, 3, 3]\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [batch_size, num_links, 3]\n",
    "    \"\"\"\n",
    "    return torch.stack([skew_tensor[:, :, 2, 1], skew_tensor[:, :, 0, 2], skew_tensor[:, :, 1, 0]], dim=2)\n",
    "\n",
    "# retarget data\n",
    "def compute_batch_jacobian(var_dof_pose, const_root_trans, const_root_pose, epsilon=1e-4):\n",
    "    \"\"\"\n",
    "    Compute the batch Jacobian matrix for both position and orientation errors.\n",
    "\n",
    "    Args:\n",
    "        var_dof_pose (torch.tensor): [1, num_frames, len(robot_joint_names), 1]\n",
    "        const_root_trans (torch.tensor): [num_frames, 3]\n",
    "        const_root_pose (torch.tensor): [num_frames, 3]\n",
    "        epsilon (float, optional): Perturbation size. Defaults to 1e-4.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - jacobian_positions (torch.tensor): [num_frames, 3 * len(robot_link_pick_idx), num_joints]\n",
    "            - jacobian_orientations (torch.tensor): [num_frames, 6, num_joints]\n",
    "    \"\"\"\n",
    "    _, num_frames, num_joints, _ = var_dof_pose.shape\n",
    "    \n",
    "    # Base pose\n",
    "    pose_aa_base = torch.cat([\n",
    "        const_root_pose[None, :, None], \n",
    "        robot_rotation_axis * var_dof_pose\n",
    "    ], dim=2)  # [1, num_frames, ...,]\n",
    "    fk_base = robot_fk.fk_batch(pose_aa_base, const_root_trans[None, :])\n",
    "    base_positions = fk_base['global_translation'][:, :, robot_link_pick_idx]  # [1, N, L, 3]\n",
    "    base_rotations = fk_base['global_rotation_mat'][:, :, hands_link_idx]  # [1, N, 2, 3, 3]\n",
    "    \n",
    "    # Initialize Jacobians\n",
    "    jacobian_positions = torch.zeros(num_frames, 3 * len(robot_link_pick_idx), num_joints, device=device)\n",
    "    jacobian_orientations = torch.zeros(num_frames, 6, num_joints, device=device)\n",
    "    \n",
    "    for j in range(num_joints):\n",
    "        # Perturb joint j by epsilon\n",
    "        var_dof_pose_perturbed = var_dof_pose.clone()\n",
    "        var_dof_pose_perturbed[0, :, j, 0] += epsilon\n",
    "        \n",
    "        pose_aa_perturbed = torch.cat([\n",
    "            const_root_pose[None, :, None], \n",
    "            robot_rotation_axis * var_dof_pose_perturbed\n",
    "        ], dim=2)  # [1, N, ...,]\n",
    "        fk_perturbed = robot_fk.fk_batch(pose_aa_perturbed, const_root_trans[None, :])\n",
    "        \n",
    "        # Position differences\n",
    "        perturbed_positions = fk_perturbed['global_translation'][:, :, robot_link_pick_idx]  # [1, N, L, 3]\n",
    "        delta_positions = (perturbed_positions - base_positions) / epsilon  # [1, N, L, 3]\n",
    "        delta_positions = delta_positions[0].reshape(num_frames, 3 * len(robot_link_pick_idx))  # [N, 3L]\n",
    "        jacobian_positions[:, :, j] = delta_positions\n",
    "        \n",
    "        # Orientation differences\n",
    "        perturbed_rotations = fk_perturbed['global_rotation_mat'][:, :, hands_link_idx]  # [1, N, 2, 3, 3]\n",
    "        delta_rot = 0.5 * (perturbed_rotations.transpose(3,4) @ base_rotations - \n",
    "                           base_rotations.transpose(3,4) @ perturbed_rotations) / epsilon  # [1, N, 2, 3, 3]\n",
    "        delta_rot = vee(delta_rot.squeeze(0))  # [1, N, 6]\n",
    "        jacobian_orientations[:, :, j] = delta_rot.view(-1, 6)\n",
    "    \n",
    "    return jacobian_positions, jacobian_orientations\n",
    "# -----------------------------------\n",
    "def compute_batch_diff(const_dof_pose, const_root_trans, const_root_pose, const_smpl_positions, \n",
    "                      left_hand_rot_desired, right_hand_rot_desired):\n",
    "    \"\"\"\n",
    "    Compute the difference in key points positions and hand orientations.\n",
    "\n",
    "    Args:\n",
    "        const_dof_pose (torch.tensor): [batch_size, num_frames, len(robot_joint_names), 1]\n",
    "        const_root_trans (torch.tensor): [num_frames, 3]\n",
    "        const_root_pose (torch.tensor): [num_frames, 3]\n",
    "        const_smpl_positions (torch.tensor): [num_frames, len(smpl_link_pick_idx), 3]\n",
    "        left_hand_rot_desired (torch.Tensor): [num_frames, 3, 3]\n",
    "        right_hand_rot_desired (torch.Tensor): [num_frames, 3, 3]\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - positions_diff (torch.tensor): [num_frames, len(robot_link_pick_idx), 3]\n",
    "            - orientations_diff (torch.tensor): [num_frames, 6] (3 for each hand)\n",
    "    \"\"\"\n",
    "    batch_size = const_dof_pose.shape[0]\n",
    "    num_frames = const_dof_pose.shape[1]\n",
    "    \n",
    "    # Compute forward kinematics\n",
    "    pose_aa = torch.cat([const_root_pose[None, :, None], robot_rotation_axis * const_dof_pose], axis = 2).to(device)\n",
    "    fk_return = robot_fk.fk_batch(pose_aa, const_root_trans[None, ])\n",
    "    \n",
    "    # Position differences\n",
    "    robot_link_positions = fk_return['global_translation'][:, :, robot_link_pick_idx]  # [B, N, len(robot_link_pick_idx), 3]\n",
    "    positions_diff = robot_link_positions - const_smpl_positions  # [B, N, len(robot_link_pick_idx), 3]\n",
    "    positions_diff = positions_diff[0]  # [N, len(robot_link_pick_idx), 3]\n",
    "    \n",
    "    # Orientation differences\n",
    "    robot_hand_rot = fk_return['global_rotation_mat'][:, :, hands_link_idx]  # [B, N, 2, 3, 3]\n",
    "    left_hand_rot = robot_hand_rot[:, :, 0]  # [B, N, 3, 3]\n",
    "    right_hand_rot = robot_hand_rot[:, :, 1]  # [B, N, 3, 3]\n",
    "    \n",
    "    # Desired orientations\n",
    "    left_diff_rot = 0.5 * (left_hand_rot_desired.unsqueeze(0).transpose(2, 3) @ left_hand_rot - \n",
    "                           left_hand_rot.transpose(2, 3) @ left_hand_rot_desired.unsqueeze(0))\n",
    "    right_diff_rot = 0.5 * (right_hand_rot_desired.unsqueeze(0).transpose(2, 3) @ right_hand_rot - \n",
    "                            right_hand_rot.transpose(2, 3) @ right_hand_rot_desired.unsqueeze(0))\n",
    "    \n",
    "    # Apply vee operator\n",
    "    left_diff = vee(left_diff_rot)  # [1, N, 3]\n",
    "    right_diff = vee(right_diff_rot)  # [1, N, 3]\n",
    "    \n",
    "    # Concatenate orientation differences\n",
    "    orientations_diff = torch.cat([left_diff, right_diff], dim=2).squeeze(0)  # [N, 6]\n",
    "    \n",
    "    return positions_diff, orientations_diff\n",
    "# -----------------------------------\n",
    "# Precompute the identity matrix once and move to the appropriate device\n",
    "robot_joint_names_length = len(robot_joint_names)\n",
    "identity_matrix = torch.eye(robot_joint_names_length, device=device)  # [num_joints, num_joints]\n",
    "\n",
    "def compute_batch_LM_step(\n",
    "    jacobian_mat, \n",
    "    diff, \n",
    "    const_last_dof_pos, \n",
    "    lambda_val=1e-3, \n",
    "    smooth_weight=1e-2\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the batch Levenberg-Marquardt step for updating DOF positions with smoothness constraints.\n",
    "\n",
    "    Args:\n",
    "        jacobian_mat (torch.Tensor): [num_frames, 3 * len(robot_link_pick_idx), num_joints]\n",
    "        diff (torch.Tensor): [num_frames, 3 * len(robot_link_pick_idx)]\n",
    "        const_last_dof_pos (torch.Tensor): [batch_size, num_frames, num_joints, 1]\n",
    "        lambda_val (float, optional): Damping factor for LM inverse kinematic. Defaults to 1e-3.\n",
    "        smooth_weight (float, optional): Weight for penalizing large changes in joint angles. Defaults to 1e-2.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [num_frames, num_joints, 1]\n",
    "    \"\"\"\n",
    "    num_frames, dim_diff, num_joints = jacobian_mat.shape\n",
    "\n",
    "    # Compute J^T J for all frames: [num_frames, num_joints, num_joints]\n",
    "    JTJ = torch.bmm(jacobian_mat.transpose(1, 2), jacobian_mat)  # [num_frames, num_joints, num_joints]\n",
    "\n",
    "    # Add damping factor and smoothness weight\n",
    "    damping = lambda_val * identity_matrix  # [num_joints, num_joints]\n",
    "    smooth_mat = smooth_weight * identity_matrix  # [num_joints, num_joints]\n",
    "\n",
    "    # Prepare smoothness weights for each frame\n",
    "    smooth_weights = smooth_mat.unsqueeze(0).repeat(num_frames, 1, 1)  # [num_frames, num_joints, num_joints]\n",
    "    smooth_weights[1:-1] += smooth_mat  # Middle frames have two neighbors\n",
    "\n",
    "    # Add damping and smoothness to JTJ\n",
    "    JTJ_damped = JTJ + smooth_weights + damping.unsqueeze(0)  # [num_frames, num_joints, num_joints]\n",
    "\n",
    "    # Construct block diagonal Hessian\n",
    "    H = torch.block_diag(*JTJ_damped)  # [num_frames*num_joints, num_frames*num_joints]\n",
    "    # Add smoothness weights to block sub-diagonal and super-diagonal\n",
    "    for i in range(1, num_frames):\n",
    "        # Add smoothness cost weight\n",
    "        H[i * num_joints : (i + 1) * num_joints, (i - 1) * num_joints : i * num_joints] = -smooth_mat\n",
    "        H[(i - 1) * num_joints : i * num_joints, i * num_joints : (i + 1) * num_joints] = -smooth_mat\n",
    "\n",
    "    # Compute J^T * diff: [num_frames, num_joints, 1]\n",
    "    JT_diff = torch.bmm(jacobian_mat.transpose(1, 2), diff.unsqueeze(-1)).squeeze(-1)  # [num_frames, num_joints]\n",
    "\n",
    "    # Add smoothness gradients\n",
    "    p = const_last_dof_pos[0].squeeze(-1)  # [num_frames, num_joints]\n",
    "    smooth_grad = torch.zeros_like(JT_diff)\n",
    "    smooth_grad[0] += smooth_weight * (p[0] - p[1])\n",
    "    smooth_grad[-1] += smooth_weight * (p[-1] - p[-2])\n",
    "    smooth_grad[1:-1] += smooth_weight * (2 * p[1:-1] - p[:-2] - p[2:])\n",
    "\n",
    "    # Combine gradients\n",
    "    grad = JT_diff + smooth_grad  # [num_frames, num_joints]\n",
    "\n",
    "    # Flatten gradient\n",
    "    grad_flat = grad.reshape(-1, 1)  # [num_frames*num_joints, 1]\n",
    "\n",
    "    # Solve H * step = grad_flat\n",
    "    step_flat = torch.linalg.solve(H, grad_flat)  # [num_frames*num_joints, 1]\n",
    "\n",
    "    # Reshape step to [num_frames, num_joints, 1]\n",
    "    step = step_flat.reshape(num_frames, num_joints, 1)\n",
    "\n",
    "    return step\n",
    "# -----------------------------------\n",
    "# parameters\n",
    "initial_lambda = 1e-1\n",
    "lambda_increase_factor = 10\n",
    "lambda_decrease_factor = 0.5\n",
    "max_lambda = 1e3\n",
    "min_lambda = 1e-2\n",
    "# Smoothness weight, for penalizing large changes in joint angles\n",
    "smooth_weight = 2e-2\n",
    "# retarget data, storing the results\n",
    "retarget_data = {}\n",
    "pbar = tqdm(train_data.keys())\n",
    "for data_key in pbar:\n",
    "    # translation\n",
    "    trans = torch.from_numpy(train_data[data_key]['trans']).float().to(device)\n",
    "    N = trans.shape[0]\n",
    "    pose_aa_walk = torch.from_numpy(np.concatenate((train_data[data_key]['pose_aa'][:, :66], np.zeros((N, 6))), axis = -1)).float().to(device)\n",
    "    # get joints, verts, and offset\n",
    "    verts, joints = smpl_parser_n.get_joints_verts(pose_aa_walk, torch.zeros((1, 10)).to(device), trans)\n",
    "    offset = joints[:, 0] - trans\n",
    "    root_trans_offset = trans * optimized_scale + offset\n",
    "    # get root rotation\n",
    "    gt_root_rot = torch.from_numpy((sRot.from_rotvec(pose_aa_walk.cpu().numpy()[:, :3]) * sRot.from_quat([0.5, 0.5, 0.5, 0.5]).inv()).as_rotvec()).float().to(device)\n",
    "    # prepare for iteration\n",
    "    dof_pos = torch.zeros((1, N, 29, 1)).to(device)\n",
    "    # set initial joint position\n",
    "    for joint_name, joint_pos in initial_joint_position_dict.items():\n",
    "        dof_pos[:, :, dict_joint_name_index[joint_name], 0] = joint_pos\n",
    "    # iterational ik optimization\n",
    "    last_loss = 1e10 # set last loss to be large enough for LM update\n",
    "    lambda_val = initial_lambda\n",
    "    # get target keypoint positions and hand orientations\n",
    "    # positions\n",
    "    verts, joints = smpl_parser_n.get_joints_verts(pose_aa_walk, shape_new, trans)\n",
    "    joints = (joints - offset.unsqueeze(1)) * optimized_scale + offset.unsqueeze(1)\n",
    "    # hand orientations\n",
    "    left_hand_rotmat, right_hand_rotmat = compute_hand_global_orientations(smpl_parser_n, pose_aa_walk) # shape: [N, 3, 3]\n",
    "    for iteration in range(5):\n",
    "        # Compute Jacobians\n",
    "        jacobian_pos, jacobian_ori = compute_batch_jacobian(dof_pos, root_trans_offset, gt_root_rot)\n",
    "        # Compute differences\n",
    "        pos_diff, ori_diff = compute_batch_diff(dof_pos, root_trans_offset, gt_root_rot, joints[:, smpl_link_pick_idx], \n",
    "                                               left_hand_rotmat, right_hand_rotmat)\n",
    "        \n",
    "        # Concatenate diffs and Jacobians\n",
    "        diff = torch.cat([pos_diff.reshape(pos_diff.shape[0], -1), ori_diff], dim=1)  # [N, 3L + 6]\n",
    "        jacobian = torch.cat([jacobian_pos, jacobian_ori], dim=1)  # [N, 3L + 6, num_joints]\n",
    "        if iteration < 5:\n",
    "            diff = pos_diff.reshape(pos_diff.shape[0], -1)  # [N, 3L]\n",
    "            jacobian = jacobian_pos\n",
    "        # diff = ori_diff  # [N, 6]\n",
    "        # jacobian = jacobian_ori\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = diff.norm(dim=1).mean()  # You might want to weight position and orientation errors differently\n",
    "        \n",
    "        # Compute LM step\n",
    "        step = compute_batch_LM_step(jacobian, diff, dof_pos, lambda_val=1e-3, smooth_weight=smooth_weight)\n",
    "        \n",
    "        # Update DOF positions\n",
    "        propose_dof_pos = (dof_pos - step).clone().clamp_(robot_fk.joints_range[:, 0, None], robot_fk.joints_range[:, 1, None])\n",
    "        pos_diff_updated, ori_diff_updated = compute_batch_diff(propose_dof_pos, root_trans_offset, gt_root_rot, joints[:, smpl_link_pick_idx], \n",
    "                                                                left_hand_rotmat, right_hand_rotmat)\n",
    "        loss_updated = torch.cat([pos_diff_updated.reshape(pos_diff_updated.shape[0], -1), ori_diff_updated], dim=1).norm(dim=1).mean()\n",
    "        \n",
    "        if loss_updated > last_loss:\n",
    "            lambda_val *= lambda_increase_factor\n",
    "            lambda_val = min(lambda_val, max_lambda)\n",
    "        else:\n",
    "            lambda_val *= lambda_decrease_factor\n",
    "            lambda_val = max(lambda_val, min_lambda)\n",
    "            last_loss = loss_updated\n",
    "            dof_pos = propose_dof_pos.clone()\n",
    "            # dof_pos.data.clamp_(robot_fk.joints_range[:, 0, None], robot_fk.joints_range[:, 1, None])\n",
    "            dof_pos[:, :, locked_joints_idx, 0] = 0.0\n",
    "    # clamp after optimization\n",
    "    dof_pos.data.clamp_(robot_fk.joints_range[:, 0, None], robot_fk.joints_range[:, 1, None])\n",
    "    # get new pose and calculate fk\n",
    "    pose_aa_new = torch.cat([gt_root_rot[None, :, None], robot_rotation_axis * dof_pos], axis = 2)\n",
    "    fk_return = robot_fk.fk_batch(pose_aa_new, root_trans_offset[None, ])\n",
    "    # save retargeted data\n",
    "    root_trans_offset_dump = root_trans_offset.clone()\n",
    "    global_translation = fk_return.global_translation.clone().squeeze(0)\n",
    "    # decrese the height by height_correction\n",
    "    height_correction = global_translation[..., 2].min().item() - 0.015    # 0.015 is the height of the robot foot\n",
    "    root_trans_offset_dump[..., 2] -= height_correction\n",
    "    global_translation[..., 2] -= height_correction\n",
    "    mocap_global_translation = joints.clone()\n",
    "    mocap_global_translation[..., 2] -= height_correction\n",
    "    # read the captions\n",
    "    def read_captions_from_file(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Each line contains a caption with its annotations, split by newline and remove empty lines\n",
    "        captions = [line.strip() for line in lines if line.strip()]\n",
    "        \n",
    "        # Extract just the first part of each line (the raw caption) before the first #\n",
    "        raw_captions = [caption.split('#')[0] for caption in captions]\n",
    "        \n",
    "        return raw_captions\n",
    "    caption_path = f\"../../data/texts/{data_key.replace('.npz', '.txt')}\"\n",
    "    captions = read_captions_from_file(caption_path)\n",
    "    \n",
    "    retarget_data[data_key] = {\n",
    "            \"root_trans_offset\": root_trans_offset_dump.squeeze().cpu().detach().numpy(),\n",
    "            \"dof\": dof_pos.squeeze().cpu().detach().numpy(),\n",
    "            \"root_rot\": sRot.from_rotvec(gt_root_rot.cpu().numpy()).as_quat(),\n",
    "            \"global_translation\": global_translation.cpu().detach(),\n",
    "            \"mocap_global_translation\": mocap_global_translation.cpu().detach(),\n",
    "            \"captions\": captions,\n",
    "            \"left_hand_rotmat\": left_hand_rotmat.cpu().detach().numpy(),\n",
    "            \"right_hand_rotmat\": right_hand_rotmat.cpu().detach().numpy(),\n",
    "    }\n",
    "    \n",
    "    # mirror the data\n",
    "    mirror_caption_path = f\"../../data/texts/M{data_key.replace('.npz', '.txt')}\"\n",
    "    mirror_caption = read_captions_from_file(mirror_caption_path)\n",
    "    \n",
    "    mirror_root_trans = root_trans_offset_dump.clone()\n",
    "    # mirror_pose_aa = pose_aa_new.clone()\n",
    "    mirror_dof = dof_pos.clone()\n",
    "    mirror_gt_root_rot = gt_root_rot.clone()\n",
    "    mirror_global_translation = global_translation.clone()\n",
    "    mirror_mocap_global_translation = mocap_global_translation.clone()\n",
    "    mirror_left_hand_rotmat = left_hand_rotmat.clone()\n",
    "    mirror_right_hand_rotmat = right_hand_rotmat.clone()\n",
    "    \n",
    "    # inverse the y axis in translation, yaw angle in rotation and dof position of symmetric joints\n",
    "    # Invert the Y-axis in translations\n",
    "    mirror_root_trans[:, 1] = -mirror_root_trans[:, 1]\n",
    "    mirror_global_translation[:, :, 1] = -mirror_global_translation[:, :, 1]\n",
    "    mirror_mocap_global_translation[:, :, 1] = -mirror_mocap_global_translation[:, :, 1]\n",
    "\n",
    "    # Adjust root rotation\n",
    "    mirror_gt_root_rot = sRot.from_rotvec(mirror_gt_root_rot.cpu().numpy()).as_matrix()\n",
    "    M = np.array([[1, 0, 0], [0, -1, 0], [0, 0, 1]])\n",
    "    mirror_gt_root_rot = M @ mirror_gt_root_rot @ M.T\n",
    "    mirror_gt_root_rot = sRot.from_matrix(mirror_gt_root_rot).as_quat()\n",
    "    \n",
    "    mirror_left_hand_rotmat = M @ mirror_left_hand_rotmat.cpu().numpy() @ M.T\n",
    "    mirror_right_hand_rotmat = M @ mirror_right_hand_rotmat.cpu().numpy() @ M.T\n",
    "    \n",
    "    \n",
    "    # Define symmetric link pairs for swapping\n",
    "    symmetric_links = [\n",
    "        ('left_hip_pitch_link', 'right_hip_pitch_link'),\n",
    "        ('left_hip_roll_link', 'right_hip_roll_link'),\n",
    "        ('left_hip_yaw_link', 'right_hip_yaw_link'),\n",
    "        ('left_knee_link', 'right_knee_link'),\n",
    "        ('left_ankle_pitch_link', 'right_ankle_pitch_link'),\n",
    "        ('left_ankle_roll_link', 'right_ankle_roll_link'),\n",
    "        ('left_shoulder_pitch_link', 'right_shoulder_pitch_link'),\n",
    "        ('left_shoulder_roll_link', 'right_shoulder_roll_link'),\n",
    "        ('left_shoulder_yaw_link', 'right_shoulder_yaw_link'),\n",
    "        ('left_elbow_link', 'right_elbow_link'),\n",
    "        ('left_wrist_roll_link', 'right_wrist_roll_link'),\n",
    "        ('left_wrist_pitch_link', 'right_wrist_pitch_link'),\n",
    "        ('left_wrist_yaw_link', 'right_wrist_yaw_link'),\n",
    "    ]\n",
    "\n",
    "    # Swap the symmetric links' global translations\n",
    "    for left_link, right_link in symmetric_links:\n",
    "        left_idx = dict_link_name_index[left_link]\n",
    "        right_idx = dict_link_name_index[right_link]\n",
    "\n",
    "        # Swap global translations\n",
    "        temp_trans = mirror_global_translation[:, left_idx, :].clone()\n",
    "        mirror_global_translation[:, left_idx, :] = mirror_global_translation[:, right_idx, :]\n",
    "        mirror_global_translation[:, right_idx, :] = temp_trans\n",
    "        \n",
    "    symmetric_smpl_links = [\n",
    "        ('L_Hip', 'R_Hip'),\n",
    "        ('L_Knee', 'R_Knee'),\n",
    "        ('L_Ankle', 'R_Ankle'),\n",
    "        ('L_Shoulder', 'R_Shoulder'),\n",
    "        ('L_Elbow', 'R_Elbow'),\n",
    "        ('L_Wrist', 'R_Wrist'),\n",
    "    ]\n",
    "    \n",
    "    # Swap the symmetric links' global translations\n",
    "    for left_link, right_link in symmetric_smpl_links:\n",
    "        left_idx = dict_smpl_link_name_index[left_link]\n",
    "        right_idx = dict_smpl_link_name_index[right_link]\n",
    "\n",
    "        # Swap mocap global translations\n",
    "        temp_mocap_trans = mirror_mocap_global_translation[:, left_idx, :].clone()\n",
    "        mirror_mocap_global_translation[:, left_idx, :] = mirror_mocap_global_translation[:, right_idx, :]\n",
    "        mirror_mocap_global_translation[:, right_idx, :] = temp_mocap_trans\n",
    "\n",
    "    # Define symmetric joint pairs for swapping\n",
    "    symmetric_joints = [\n",
    "        ('left_shoulder_pitch_joint', 'right_shoulder_pitch_joint'),\n",
    "        ('left_shoulder_roll_joint', 'right_shoulder_roll_joint'),\n",
    "        ('left_shoulder_yaw_joint', 'right_shoulder_yaw_joint'),\n",
    "        ('left_elbow_joint', 'right_elbow_joint'),\n",
    "        ('left_wrist_roll_joint', 'right_wrist_roll_joint'),\n",
    "        ('left_wrist_pitch_joint', 'right_wrist_pitch_joint'),\n",
    "        ('left_wrist_yaw_joint', 'right_wrist_yaw_joint'),\n",
    "        {'left_hip_pitch_joint', 'right_hip_pitch_joint'},\n",
    "        {'left_hip_roll_joint', 'right_hip_roll_joint'},\n",
    "        {'left_hip_yaw_joint', 'right_hip_yaw_joint'},\n",
    "        {'left_knee_joint', 'right_knee_joint'},\n",
    "        {'left_ankle_pitch_joint', 'right_ankle_pitch_joint'},\n",
    "        {'left_ankle_roll_joint', 'right_ankle_roll_joint'},\n",
    "    ]\n",
    "\n",
    "    # Swap the DOF positions and negate yaw angles for symmetric joints\n",
    "    for left_joint, right_joint in symmetric_joints:\n",
    "        left_idx = dict_joint_name_index[left_joint]\n",
    "        right_idx = dict_joint_name_index[right_joint]\n",
    "\n",
    "        # Store the left joint DOF temporarily\n",
    "        temp = mirror_dof[:, :, left_idx, 0].clone()\n",
    "\n",
    "        # Swap DOF positions between left and right joints\n",
    "        mirror_dof[:, :, left_idx, 0] = mirror_dof[:, :, right_idx, 0]\n",
    "        mirror_dof[:, :, right_idx, 0] = temp\n",
    "\n",
    "        # Negate the yaw angles for both joints if they have yaw\n",
    "        if 'yaw_joint' in left_joint or 'roll_joint' in left_joint:\n",
    "            mirror_dof[:, :, left_idx, 0] = -mirror_dof[:, :, left_idx, 0]\n",
    "            mirror_dof[:, :, right_idx, 0] = -mirror_dof[:, :, right_idx, 0]\n",
    "\n",
    "    # Swap the left and right hand rotation matrices\n",
    "    mirror_left_hand_rotmat, mirror_right_hand_rotmat = deepcopy(mirror_right_hand_rotmat), deepcopy(mirror_left_hand_rotmat)\n",
    "    #\n",
    "    # Save the mirrored data with a prefixed key 'M'\n",
    "    retarget_data['M' + data_key] = {\n",
    "        \"root_trans_offset\": mirror_root_trans.squeeze().cpu().detach().numpy(),\n",
    "        \"dof\": mirror_dof.squeeze().cpu().detach().numpy(),\n",
    "        \"root_rot\": mirror_gt_root_rot,\n",
    "        \"global_translation\": mirror_global_translation.cpu().detach(),\n",
    "        \"mocap_global_translation\": mirror_mocap_global_translation.cpu().detach(),\n",
    "        \"captions\": mirror_caption,\n",
    "        \"left_hand_rotmat\": mirror_left_hand_rotmat,\n",
    "        \"right_hand_rotmat\": mirror_right_hand_rotmat,\n",
    "    }\n",
    "    \n",
    "    \n",
    "# configuration information\n",
    "retarget_data['config'] = {\n",
    "    \"frequency\": target_frequency,\n",
    "    \"dict_joint_name_index\" : dict_joint_name_index,\n",
    "    \"dict_link_name_index\" : dict_link_name_index,\n",
    "    \"list_selected_links\" : list_selected_links,\n",
    "    \"list_selected_joints\" : list_selected_joints,\n",
    "    \"left_foot_name\" : \"left_ankle_roll_link\",\n",
    "    \"right_foot_name\" : \"right_ankle_roll_link\",\n",
    "} \n",
    "# save retargeted data\n",
    "joblib.dump(retarget_data, f'../../data/g1/humanml3d_train_retargeted_wholebody_{len(train_data.keys())}.pkl')\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smooth-mocap-retarget",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
